import google.generativeai as genai
import os
from app.src.config import get_settings
from app.src.utils.logs import logger

class LLMOrchestrator:
    def __init__(self):
        self.settings = get_settings()
        if self.settings.gemini_api_key:
            genai.configure(api_key=self.settings.gemini_api_key)
            self.model = genai.GenerativeModel(self.settings.gemini_model)
            logger.info("LLM orchestrator initialized with Gemini model")
        else:
            self.model = None
            logger.error("GEMINI_API_KEY not configured for LLM orchestrator")

    def build_prompt(self, query: str, context: str, chat_history_context: str = "") -> str:
        try:
            logger.debug(f"Building prompt - Query length: {len(query)}, Context length: {len(context)}, History context length: {len(chat_history_context)}")

            if not query or not query.strip():
                logger.error("Empty or invalid query provided for prompt building")
                raise ValueError("Query cannot be empty for prompt building")

            if not context or not context.strip():
                logger.warning("Empty or minimal context provided for prompt building")

            system_prompt_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                "services/chat/system_prompts/system_prompt.txt"
            )

            if not os.path.exists(system_prompt_path):
                logger.error(f"System prompt file not found: {system_prompt_path}")
                raise FileNotFoundError(f"System prompt file not found: {system_prompt_path}")

            with open(system_prompt_path, 'r', encoding='utf-8') as file:
                system_prompt_template = file.read()

            if not system_prompt_template.strip():
                logger.error("System prompt template is empty")
                raise ValueError("System prompt template is empty")

            system_prompt = system_prompt_template.replace("# Context:", f"# Context:\n\n{context}")

            if chat_history_context:
                system_prompt = system_prompt.replace("# User Query:", f"{chat_history_context}\n# User Query:")

            system_prompt = system_prompt.replace("# User Query:", f"# User Query:\n\n{query}")

            logger.debug("Prompt built successfully with chat history context")
            return system_prompt

        except FileNotFoundError as e:
            logger.error(f"File not found error in prompt building: {str(e)}")
            raise
        except ValueError as e:
            logger.error(f"Validation error in prompt building: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error building prompt: {str(e)}")
            raise

    def generate(self, prompt: str) -> str:
        try:
            logger.info("Starting LLM generation")

            if not self.model:
                error_msg = "Error: Gemini API not configured. Please set GEMINI_API_KEY environment variable."
                logger.error("Gemini model not available for generation")
                raise RuntimeError(error_msg)

            if not prompt or not prompt.strip():
                logger.error("Empty or invalid prompt provided for generation")
                raise ValueError("Prompt cannot be empty for generation")

            logger.debug(f"Sending request to Gemini - Prompt length: {len(prompt)}")

            response = self.model.generate_content(
                prompt,
                generation_config={"max_output_tokens": self.settings.answer_max_tokens}
            )

            if not response:
                logger.error("No response object received from Gemini")
                raise RuntimeError("No response received from Gemini model")

            if not response.text:
                logger.error("No text content in Gemini response")
                raise RuntimeError("No response text generated by Gemini model")

            generated_text = response.text.strip()

            if not generated_text:
                logger.error("Empty response text generated by Gemini")
                raise RuntimeError("Empty response generated by Gemini model")

            logger.info(f"LLM generation completed successfully - Response length: {len(generated_text)}")
            return generated_text

        except ValueError as e:
            logger.error(f"Validation error in LLM generation: {str(e)}")
            raise
        except RuntimeError as e:
            logger.error(f"Runtime error in LLM generation: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in LLM generation: {str(e)}")
            raise RuntimeError(f"LLM generation failed: {str(e)}")
